From 6ddb58ef48cb2615daee379bb9ee3d3cbf1e6918 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Thu, 11 Apr 2024 10:46:51 +0200
Subject: [PATCH 1/7] !35783

nir: remove recursive check in nir_lower_phis_to_scalar()

This check causes unnecessary overhead and it makes little sense
that phis are considered scalarizable if they only feed into themselves.

nir/lower_phis_to_scalar: remove exec_list dead_instrs

Just free the instructions directly.

nir: add callback function to nir_lower_phis_to_scalar()
---
 src/amd/common/nir/ac_nir_lower_ngg.c         |   2 +-
 src/amd/common/nir/ac_nir_lower_ngg_mesh.c    |   2 +-
 .../common/nir/ac_nir_lower_tess_io_to_mem.c  |   2 +-
 .../instruction_selection/aco_isel_setup.cpp  |   2 +-
 src/amd/vulkan/radv_shader.c                  |   2 +-
 src/asahi/clc/asahi_clc.c                     |   2 +-
 src/asahi/compiler/agx_compile.c              |   4 +-
 src/broadcom/compiler/nir_to_vir.c            |   2 +-
 src/compiler/glsl/gl_nir_linker.c             |   2 +-
 src/compiler/nir/nir.h                        |   3 +-
 src/compiler/nir/nir_lower_phis_to_scalar.c   | 114 +++++++++---------
 src/compiler/spirv/vtn_bindgen2.c             |   2 +-
 src/freedreno/ir3/ir3_nir.c                   |   2 +-
 .../auxiliary/gallivm/lp_bld_nir_soa.c        |   2 +-
 src/gallium/auxiliary/nir/tgsi_to_nir.c       |   2 +-
 src/gallium/drivers/lima/lima_program.c       |   2 +-
 .../nouveau/codegen/nv50_ir_from_nir.cpp      |   2 +-
 src/gallium/drivers/r600/sfn/sfn_nir.cpp      |   8 +-
 src/gallium/drivers/radeonsi/si_shader_nir.c  |   4 +-
 src/gallium/drivers/vc4/vc4_program.c         |   2 +-
 src/gallium/drivers/zink/zink_compiler.c      |   4 +-
 src/gallium/frontends/rusticl/core/kernel.rs  |   2 +-
 src/intel/compiler/brw_nir.c                  |   2 +-
 src/intel/compiler/brw_spirv.c                |   2 +-
 src/intel/compiler/elk/elk_nir.c              |   2 +-
 src/intel/compiler/elk/elk_spirv.c            |   2 +-
 src/mesa/state_tracker/st_glsl_to_nir.cpp     |   2 +-
 src/microsoft/compiler/nir_to_dxil.c          |   2 +-
 src/nouveau/compiler/nak_nir.c                |   2 +-
 src/panfrost/clc/pan_compile.c                |   2 +-
 src/panfrost/compiler/bifrost_compile.c       |   2 +-
 31 files changed, 92 insertions(+), 95 deletions(-)

diff --git a/src/amd/common/nir/ac_nir_lower_ngg.c b/src/amd/common/nir/ac_nir_lower_ngg.c
index 5c7a6bcc75cd7..b3d0801ae4842 100644
--- a/src/amd/common/nir/ac_nir_lower_ngg.c
+++ b/src/amd/common/nir/ac_nir_lower_ngg.c
@@ -1838,7 +1838,7 @@ ac_nir_lower_ngg_nogs(nir_shader *shader, const ac_nir_lower_ngg_options *option
    nir_lower_vars_to_ssa(shader);
    nir_remove_dead_variables(shader, nir_var_function_temp, NULL);
    nir_lower_alu_to_scalar(shader, NULL, NULL);
-   nir_lower_phis_to_scalar(shader, true);
+   nir_lower_all_phis_to_scalar(shader);
 
    if (options->can_cull) {
       /* It's beneficial to redo these opts after splitting the shader. */
diff --git a/src/amd/common/nir/ac_nir_lower_ngg_mesh.c b/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
index 3cbe41e020197..15c51f512398b 100644
--- a/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
+++ b/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
@@ -1437,7 +1437,7 @@ ac_nir_lower_ngg_mesh(nir_shader *shader,
    nir_lower_vars_to_ssa(shader);
    nir_remove_dead_variables(shader, nir_var_function_temp, NULL);
    nir_lower_alu_to_scalar(shader, NULL, NULL);
-   nir_lower_phis_to_scalar(shader, true);
+   nir_lower_all_phis_to_scalar(shader);
 
    /* Optimize load_local_invocation_index. When the API workgroup is smaller than the HW workgroup,
     * local_invocation_id isn't initialized for all lanes and we can't perform this optimization for
diff --git a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
index e2bb74eca8c64..7a160023a1753 100644
--- a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
+++ b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
@@ -1624,7 +1624,7 @@ ac_nir_lower_hs_outputs_to_mem(nir_shader *shader, const nir_tcs_info *info,
    NIR_PASS(_, shader, nir_lower_vars_to_ssa);
    NIR_PASS(_, shader, nir_remove_dead_variables, nir_var_function_temp, NULL);
    NIR_PASS(_, shader, nir_lower_alu_to_scalar, NULL, NULL);
-   NIR_PASS(_, shader, nir_lower_phis_to_scalar, true);
+   NIR_PASS(_, shader, nir_lower_all_phis_to_scalar);
 
    return true;
 }
diff --git a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
index 6ca9d7b1b84f8..22d3c2e14e6a5 100644
--- a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
+++ b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
@@ -251,7 +251,7 @@ void
 setup_nir(isel_context* ctx, nir_shader* nir)
 {
    nir_convert_to_lcssa(nir, true, false);
-   if (nir_lower_phis_to_scalar(nir, true)) {
+   if (nir_lower_all_phis_to_scalar(nir)) {
       nir_copy_prop(nir);
       nir_opt_dce(nir);
    }
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 43de4953b995c..36ede34e8df15 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -181,7 +181,7 @@ radv_optimize_nir(struct nir_shader *shader, bool optimize_conservatively)
       NIR_LOOP_PASS(_, skip, shader, nir_lower_vars_to_ssa);
 
       NIR_LOOP_PASS(_, skip, shader, nir_lower_alu_width, vectorize_vec2_16bit, NULL);
-      NIR_LOOP_PASS(_, skip, shader, nir_lower_phis_to_scalar, true);
+      NIR_LOOP_PASS(_, skip, shader, nir_lower_all_phis_to_scalar);
 
       NIR_LOOP_PASS(progress, skip, shader, nir_copy_prop);
       NIR_LOOP_PASS(progress, skip, shader, nir_opt_remove_phis);
diff --git a/src/asahi/clc/asahi_clc.c b/src/asahi/clc/asahi_clc.c
index ad460647cc295..7118d06b66ca1 100644
--- a/src/asahi/clc/asahi_clc.c
+++ b/src/asahi/clc/asahi_clc.c
@@ -55,7 +55,7 @@ optimize(nir_shader *nir)
 
       NIR_PASS(progress, nir, nir_copy_prop);
       NIR_PASS(progress, nir, nir_opt_remove_phis);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, nir, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, nir, nir_opt_dce);
       NIR_PASS(progress, nir, nir_opt_dead_cf);
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/asahi/compiler/agx_compile.c b/src/asahi/compiler/agx_compile.c
index 5a9d6d68c0a4a..fd06e91dbf59a 100644
--- a/src/asahi/compiler/agx_compile.c
+++ b/src/asahi/compiler/agx_compile.c
@@ -3205,7 +3205,7 @@ agx_optimize_nir(nir_shader *nir, bool soft_fault, uint16_t *preamble_size)
 
    NIR_PASS(_, nir, nir_opt_sink, move_all);
    NIR_PASS(_, nir, nir_opt_move, move_all);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, true);
+   NIR_PASS(_, nir, nir_lower_all_phis_to_scalar);
 }
 
 /*
@@ -3792,7 +3792,7 @@ agx_preprocess_nir(nir_shader *nir)
    NIR_PASS(_, nir, nir_shader_intrinsics_pass, agx_lower_front_face,
             nir_metadata_control_flow, NULL);
    NIR_PASS(_, nir, agx_nir_lower_subgroups);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, true);
+   NIR_PASS(_, nir, nir_lower_all_phis_to_scalar);
    NIR_PASS(_, nir, nir_shader_alu_pass, agx_nir_lower_fdiv,
             nir_metadata_control_flow, NULL);
 
diff --git a/src/broadcom/compiler/nir_to_vir.c b/src/broadcom/compiler/nir_to_vir.c
index 8efcc80bf09d4..6538436f217e2 100644
--- a/src/broadcom/compiler/nir_to_vir.c
+++ b/src/broadcom/compiler/nir_to_vir.c
@@ -2142,7 +2142,7 @@ v3d_optimize_nir(struct v3d_compile *c, struct nir_shader *s)
                          NULL);
 
                 NIR_PASS(progress, s, nir_lower_alu_to_scalar, NULL, NULL);
-                NIR_PASS(progress, s, nir_lower_phis_to_scalar, false);
+                NIR_PASS(progress, s, nir_lower_phis_to_scalar, NULL, NULL);
                 NIR_PASS(progress, s, nir_copy_prop);
                 NIR_PASS(progress, s, nir_opt_remove_phis);
                 NIR_PASS(progress, s, nir_opt_dce);
diff --git a/src/compiler/glsl/gl_nir_linker.c b/src/compiler/glsl/gl_nir_linker.c
index 168bd254d3f7f..315bd0933e9ed 100644
--- a/src/compiler/glsl/gl_nir_linker.c
+++ b/src/compiler/glsl/gl_nir_linker.c
@@ -69,7 +69,7 @@ gl_nir_opts(nir_shader *nir)
       if (nir->options->lower_to_scalar) {
          NIR_PASS(_, nir, nir_lower_alu_to_scalar,
                     nir->options->lower_to_scalar_filter, NULL);
-         NIR_PASS(_, nir, nir_lower_phis_to_scalar, false);
+         NIR_PASS(_, nir, nir_lower_phis_to_scalar, NULL, NULL);
       }
 
       NIR_PASS(_, nir, nir_lower_alu);
diff --git a/src/compiler/nir/nir.h b/src/compiler/nir/nir.h
index 03151ed5e41b8..b916b3c85176e 100644
--- a/src/compiler/nir/nir.h
+++ b/src/compiler/nir/nir.h
@@ -5303,7 +5303,8 @@ bool nir_lower_alu_conversion_to_intrinsic(nir_shader *shader);
 bool nir_lower_int_to_float(nir_shader *shader);
 bool nir_lower_load_const_to_scalar(nir_shader *shader);
 bool nir_lower_read_invocation_to_scalar(nir_shader *shader);
-bool nir_lower_phis_to_scalar(nir_shader *shader, bool lower_all);
+bool nir_lower_phis_to_scalar(nir_shader *shader, nir_vectorize_cb cb, const void *data);
+bool nir_lower_all_phis_to_scalar(nir_shader *shader);
 void nir_lower_io_array_vars_to_elements(nir_shader *producer, nir_shader *consumer);
 bool nir_lower_io_array_vars_to_elements_no_indirects(nir_shader *shader,
                                                       bool outputs_only);
diff --git a/src/compiler/nir/nir_lower_phis_to_scalar.c b/src/compiler/nir/nir_lower_phis_to_scalar.c
index 7a96a02db81eb..06746c6f342de 100644
--- a/src/compiler/nir/nir_lower_phis_to_scalar.c
+++ b/src/compiler/nir/nir_lower_phis_to_scalar.c
@@ -32,20 +32,11 @@
 struct lower_phis_to_scalar_state {
    nir_shader *shader;
    nir_builder builder;
-   struct exec_list dead_instrs;
 
-   bool lower_all;
-
-   /* Hash table marking which phi nodes are scalarizable.  The key is
-    * pointers to phi instructions and the entry is either NULL for not
-    * scalarizable or non-null for scalarizable.
-    */
-   struct hash_table *phi_table;
+   nir_vectorize_cb cb;
+   const void *data;
 };
 
-static bool
-should_lower_phi(nir_phi_instr *phi, struct lower_phis_to_scalar_state *state);
-
 static bool
 is_phi_src_scalarizable(nir_phi_src *src,
                         struct lower_phis_to_scalar_state *state)
@@ -66,8 +57,10 @@ is_phi_src_scalarizable(nir_phi_src *src,
    }
 
    case nir_instr_type_phi:
-      /* A phi is scalarizable if we're going to lower it */
-      return should_lower_phi(nir_instr_as_phi(src_instr), state);
+      /* For a phi to be scalarizable, it needs some other other src than
+       * just more phis. Since we OR the result, a phi that gets scalarized,
+       * cascades down to other phi-uses as well. */
+      return false;
 
    case nir_instr_type_load_const:
       /* These are trivially scalarizable */
@@ -144,20 +137,8 @@ should_lower_phi(nir_phi_instr *phi, struct lower_phis_to_scalar_state *state)
    if (phi->def.num_components == 1)
       return false;
 
-   if (state->lower_all)
-      return true;
-
-   struct hash_entry *entry = _mesa_hash_table_search(state->phi_table, phi);
-   if (entry)
-      return entry->data != NULL;
-
-   /* Insert an entry and mark it as scalarizable for now. That way
-    * we don't recurse forever and a cycle in the dependence graph
-    * won't automatically make us fail to scalarize.
-    */
-   entry = _mesa_hash_table_insert(state->phi_table, phi, (void *)(intptr_t)1);
-
-   bool scalarizable = false;
+   if (state->cb)
+      return state->cb(&phi->instr, state->data);
 
    nir_foreach_phi_src(src, phi) {
       /* This loop ignores srcs that are not scalarizable because its likely
@@ -165,19 +146,11 @@ should_lower_phi(nir_phi_instr *phi, struct lower_phis_to_scalar_state *state)
        * This reduces register spilling by a huge amount in the i965 driver for
        * Deus Ex: MD.
        */
-      scalarizable = is_phi_src_scalarizable(src, state);
-      if (scalarizable)
-         break;
+      if (is_phi_src_scalarizable(src, state))
+         return true;
    }
 
-   /* The hash table entry for 'phi' may have changed while recursing the
-    * dependence graph, so we need to reset it */
-   entry = _mesa_hash_table_search(state->phi_table, phi);
-   assert(entry);
-
-   entry->data = (void *)(intptr_t)scalarizable;
-
-   return scalarizable;
+   return false;
 }
 
 static bool
@@ -191,22 +164,33 @@ lower_phis_to_scalar_block(nir_block *block,
     * we're modifying the linked list of instructions.
     */
    nir_foreach_phi_safe(phi, block) {
-      if (!should_lower_phi(phi, state))
+      /* Already scalar */
+      if (phi->def.num_components == 1)
+         continue;
+
+      unsigned target_width = 0;
+      unsigned num_components = phi->def.num_components;
+      if (state->cb)
+         target_width = state->cb(&phi->instr, state->data);
+      else if (should_lower_phi(phi, state))
+         target_width = 1;
+
+      if (target_width == 0 || num_components <= target_width)
          continue;
 
       /* Create a vecN operation to combine the results.  Most of these
        * will be redundant, but copy propagation should clean them up for
        * us.  No need to add the complexity here.
        */
-      nir_def *vec_srcs[NIR_MAX_VEC_COMPONENTS];
+      nir_alu_instr *vec = nir_alu_instr_create(state->builder.shader,
+                                                nir_op_vec(num_components));
 
-      for (unsigned i = 0; i < phi->def.num_components; i++) {
+      for (unsigned chan = 0; chan < num_components; chan += target_width) {
+         unsigned components = MIN2(target_width, num_components - chan);
          nir_phi_instr *new_phi = nir_phi_instr_create(state->shader);
-         nir_def_init(&new_phi->instr, &new_phi->def, 1,
+         nir_def_init(&new_phi->instr, &new_phi->def, components,
                       phi->def.bit_size);
 
-         vec_srcs[i] = &new_phi->def;
-
          nir_foreach_phi_src(src, phi) {
             nir_def *def;
             state->builder.cursor = nir_after_block_before_jump(src->pred);
@@ -216,23 +200,28 @@ lower_phis_to_scalar_block(nir_block *block,
                 * original one. This makes it easier for other passes to
                 * detect undefs without having to chase moves.
                 */
-               def = nir_undef(&state->builder, 1, phi->def.bit_size);
+               def = nir_undef(&state->builder, components, phi->def.bit_size);
             } else {
                /* We need to insert a mov to grab the i'th component of src */
-               def = nir_channel(&state->builder, src->src.ssa, i);
+               def = nir_channels(&state->builder, src->src.ssa,
+                                  nir_component_mask(components) << chan);
             }
 
             nir_phi_instr_add_src(new_phi, src->pred, def);
          }
 
+         for (unsigned i = 0; i < components; i++) {
+            vec->src[chan + i].src = nir_src_for_ssa(&new_phi->def);
+            vec->src[chan + i].swizzle[0] = i;
+         }
          nir_instr_insert_before(&phi->instr, &new_phi->instr);
       }
 
       state->builder.cursor = nir_after_phis(block);
-      nir_def *vec = nir_vec(&state->builder, vec_srcs, phi->def.num_components);
+      nir_builder_alu_instr_finish_and_insert(&state->builder, vec);
 
-      nir_def_replace(&phi->def, vec);
-      exec_list_push_tail(&state->dead_instrs, &phi->instr.node);
+      nir_def_replace(&phi->def, &vec->def);
+      nir_instr_free(&phi->instr);
 
       progress = true;
 
@@ -250,16 +239,15 @@ lower_phis_to_scalar_block(nir_block *block,
 }
 
 static bool
-lower_phis_to_scalar_impl(nir_function_impl *impl, bool lower_all)
+lower_phis_to_scalar_impl(nir_function_impl *impl, nir_vectorize_cb cb, const void *data)
 {
    struct lower_phis_to_scalar_state state;
    bool progress = false;
 
    state.shader = impl->function->shader;
    state.builder = nir_builder_create(impl);
-   exec_list_make_empty(&state.dead_instrs);
-   state.phi_table = _mesa_pointer_hash_table_create(NULL);
-   state.lower_all = lower_all;
+   state.cb = cb;
+   state.data = data;
 
    nir_foreach_block(block, impl) {
       progress = lower_phis_to_scalar_block(block, &state) || progress;
@@ -267,10 +255,6 @@ lower_phis_to_scalar_impl(nir_function_impl *impl, bool lower_all)
 
    nir_progress(true, impl, nir_metadata_control_flow);
 
-   nir_instr_free_list(&state.dead_instrs);
-
-   ralloc_free(state.phi_table);
-
    return progress;
 }
 
@@ -282,13 +266,25 @@ lower_phis_to_scalar_impl(nir_function_impl *impl, bool lower_all)
  * don't bother lowering because that would generate hard-to-coalesce movs.
  */
 bool
-nir_lower_phis_to_scalar(nir_shader *shader, bool lower_all)
+nir_lower_phis_to_scalar(nir_shader *shader, nir_vectorize_cb cb, const void *data)
 {
    bool progress = false;
 
    nir_foreach_function_impl(impl, shader) {
-      progress = lower_phis_to_scalar_impl(impl, lower_all) || progress;
+      progress = lower_phis_to_scalar_impl(impl, cb, data) || progress;
    }
 
    return progress;
 }
+
+static uint8_t
+lower_all_phis(const nir_instr *phi, const void *_)
+{
+   return 1;
+}
+
+bool
+nir_lower_all_phis_to_scalar(nir_shader *shader)
+{
+   return nir_lower_phis_to_scalar(shader, lower_all_phis, NULL);
+}
\ No newline at end of file
diff --git a/src/compiler/spirv/vtn_bindgen2.c b/src/compiler/spirv/vtn_bindgen2.c
index 94e23082b721e..7a1381e8f7531 100644
--- a/src/compiler/spirv/vtn_bindgen2.c
+++ b/src/compiler/spirv/vtn_bindgen2.c
@@ -92,7 +92,7 @@ optimize(nir_shader *nir)
 
       NIR_PASS(progress, nir, nir_copy_prop);
       NIR_PASS(progress, nir, nir_opt_remove_phis);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, nir, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, nir, nir_opt_dce);
       NIR_PASS(progress, nir, nir_opt_dead_cf);
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/freedreno/ir3/ir3_nir.c b/src/freedreno/ir3/ir3_nir.c
index e2ad5fe736ba2..e1addde12b0e7 100644
--- a/src/freedreno/ir3/ir3_nir.c
+++ b/src/freedreno/ir3/ir3_nir.c
@@ -324,7 +324,7 @@ ir3_optimize_loop(struct ir3_compiler *compiler,
 
       OPT(s, nir_lower_vars_to_ssa);
       progress |= OPT(s, nir_lower_alu_to_scalar, NULL, NULL);
-      progress |= OPT(s, nir_lower_phis_to_scalar, false);
+      progress |= OPT(s, nir_lower_phis_to_scalar, NULL, NULL);
 
       progress |= OPT(s, nir_copy_prop);
       progress |= OPT(s, nir_opt_deref);
diff --git a/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c b/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
index 6648e7ed697ec..2e0f44b8a8098 100644
--- a/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
+++ b/src/gallium/auxiliary/gallivm/lp_bld_nir_soa.c
@@ -6126,7 +6126,7 @@ lp_build_nir_soa_prepasses(struct nir_shader *nir)
    NIR_PASS(_, nir, nir_lower_load_const_to_scalar);
 
    NIR_PASS(_, nir, nir_convert_to_lcssa, false, false);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, true);
+   NIR_PASS(_, nir, nir_lower_all_phis_to_scalar);
 
    bool progress;
    do {
diff --git a/src/gallium/auxiliary/nir/tgsi_to_nir.c b/src/gallium/auxiliary/nir/tgsi_to_nir.c
index 7d7b6727755d8..7a2c7e9a65816 100644
--- a/src/gallium/auxiliary/nir/tgsi_to_nir.c
+++ b/src/gallium/auxiliary/nir/tgsi_to_nir.c
@@ -2400,7 +2400,7 @@ ttn_optimize_nir(nir_shader *nir)
       if (nir->options->lower_to_scalar) {
          NIR_PASS(progress, nir, nir_lower_alu_to_scalar,
                     nir->options->lower_to_scalar_filter, NULL);
-         NIR_PASS(progress, nir, nir_lower_phis_to_scalar, false);
+         NIR_PASS(progress, nir, nir_lower_phis_to_scalar, NULL, NULL);
       }
 
       NIR_PASS(progress, nir, nir_lower_alu);
diff --git a/src/gallium/drivers/lima/lima_program.c b/src/gallium/drivers/lima/lima_program.c
index f9fb395a02eb9..3a2b1de5c8017 100644
--- a/src/gallium/drivers/lima/lima_program.c
+++ b/src/gallium/drivers/lima/lima_program.c
@@ -129,7 +129,7 @@ lima_program_optimize_vs_nir(struct nir_shader *s)
 
       NIR_PASS_V(s, nir_lower_vars_to_ssa);
       NIR_PASS(progress, s, nir_lower_alu_to_scalar, NULL, NULL);
-      NIR_PASS(progress, s, nir_lower_phis_to_scalar, false);
+      NIR_PASS(progress, s, nir_lower_phis_to_scalar, NULL, NULL);
       NIR_PASS(progress, s, nir_copy_prop);
       NIR_PASS(progress, s, nir_opt_remove_phis);
       NIR_PASS(progress, s, nir_opt_dce);
diff --git a/src/gallium/drivers/nouveau/codegen/nv50_ir_from_nir.cpp b/src/gallium/drivers/nouveau/codegen/nv50_ir_from_nir.cpp
index 74226afdd1e87..463c6c19d8fad 100644
--- a/src/gallium/drivers/nouveau/codegen/nv50_ir_from_nir.cpp
+++ b/src/gallium/drivers/nouveau/codegen/nv50_ir_from_nir.cpp
@@ -3443,7 +3443,7 @@ Converter::run()
 
    NIR_PASS(_, nir, nir_lower_load_const_to_scalar);
    NIR_PASS(_, nir, nir_lower_alu_to_scalar, NULL, NULL);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, false);
+   NIR_PASS(_, nir, nir_lower_phis_to_scalar, NULL, NULL);
 
    NIR_PASS(_, nir, nir_lower_frexp);
 
diff --git a/src/gallium/drivers/r600/sfn/sfn_nir.cpp b/src/gallium/drivers/r600/sfn/sfn_nir.cpp
index 10ec0b0dbb02f..ac9c772fc9a09 100644
--- a/src/gallium/drivers/r600/sfn/sfn_nir.cpp
+++ b/src/gallium/drivers/r600/sfn/sfn_nir.cpp
@@ -861,7 +861,7 @@ r600_finalize_nir_common(nir_shader *nir, enum amd_gfx_level gfx_level)
    NIR_PASS(_, nir, nir_lower_idiv, &idiv_options);
 
    NIR_PASS(_, nir, r600_nir_lower_trigen, gfx_level);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, false);
+   NIR_PASS(_, nir, nir_lower_phis_to_scalar, NULL, NULL);
    NIR_PASS(_, nir, nir_lower_undef_to_zero);
 
    struct nir_lower_tex_options lower_tex_options = {0};
@@ -957,11 +957,11 @@ r600_lower_and_optimize_nir(nir_shader *sh,
    NIR_PASS(_, sh, nir_io_add_const_offset_to_base, io_modes);
 
    NIR_PASS(_, sh, nir_lower_alu_to_scalar, r600_lower_to_scalar_instr_filter, NULL);
-   NIR_PASS(_, sh, nir_lower_phis_to_scalar, false);
+   NIR_PASS(_, sh, nir_lower_phis_to_scalar, NULL, NULL);
    if (lower_64bit)
       NIR_PASS(_, sh, r600::r600_nir_split_64bit_io);
    NIR_PASS(_, sh, nir_lower_alu_to_scalar, r600_lower_to_scalar_instr_filter, NULL);
-   NIR_PASS(_, sh, nir_lower_phis_to_scalar, false);
+   NIR_PASS(_, sh, nir_lower_phis_to_scalar, NULL, NULL);
    NIR_PASS(_, sh, nir_lower_alu_to_scalar, r600_lower_to_scalar_instr_filter, NULL);
    NIR_PASS(_, sh, nir_copy_prop);
    NIR_PASS(_, sh, nir_opt_dce);
@@ -989,7 +989,7 @@ r600_lower_and_optimize_nir(nir_shader *sh,
    }
 
    NIR_PASS(_, sh, nir_lower_alu_to_scalar, r600_lower_to_scalar_instr_filter, NULL);
-   NIR_PASS(_, sh, nir_lower_phis_to_scalar, false);
+   NIR_PASS(_, sh, nir_lower_phis_to_scalar, NULL, NULL);
    NIR_PASS(_, sh, nir_lower_alu_to_scalar, r600_lower_to_scalar_instr_filter, NULL);
    NIR_PASS(_, sh, r600_nir_lower_int_tg4);
    NIR_PASS(_, sh, r600::r600_nir_lower_tex_to_backend, gfx_level);
diff --git a/src/gallium/drivers/radeonsi/si_shader_nir.c b/src/gallium/drivers/radeonsi/si_shader_nir.c
index e45a65e3e58d9..b1d57d6abf92a 100644
--- a/src/gallium/drivers/radeonsi/si_shader_nir.c
+++ b/src/gallium/drivers/radeonsi/si_shader_nir.c
@@ -53,7 +53,7 @@ void si_nir_opts(struct si_screen *sscreen, struct nir_shader *nir, bool has_arr
 
       NIR_PASS(progress, nir, nir_lower_vars_to_ssa);
       NIR_PASS(progress, nir, nir_lower_alu_to_scalar, nir->options->lower_to_scalar_filter, NULL);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, false);
+      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, NULL, NULL);
 
       if (has_array_temps) {
          NIR_PASS(progress, nir, nir_split_array_vars, nir_var_function_temp);
@@ -77,7 +77,7 @@ void si_nir_opts(struct si_screen *sscreen, struct nir_shader *nir, bool has_arr
          NIR_PASS_V(nir, nir_lower_alu_to_scalar, nir->options->lower_to_scalar_filter, NULL);
       }
       if (lower_phis_to_scalar)
-         NIR_PASS_V(nir, nir_lower_phis_to_scalar, false);
+         NIR_PASS_V(nir, nir_lower_phis_to_scalar, NULL, NULL);
       progress |= lower_alu_to_scalar | lower_phis_to_scalar;
 
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/gallium/drivers/vc4/vc4_program.c b/src/gallium/drivers/vc4/vc4_program.c
index f4dc50a1b2445..4139d15900cb8 100644
--- a/src/gallium/drivers/vc4/vc4_program.c
+++ b/src/gallium/drivers/vc4/vc4_program.c
@@ -1496,7 +1496,7 @@ vc4_optimize_nir(struct nir_shader *s)
 
                 NIR_PASS(_, s, nir_lower_vars_to_ssa);
                 NIR_PASS(progress, s, nir_lower_alu_to_scalar, NULL, NULL);
-                NIR_PASS(progress, s, nir_lower_phis_to_scalar, false);
+                NIR_PASS(progress, s, nir_lower_phis_to_scalar, NULL, NULL);
                 NIR_PASS(progress, s, nir_copy_prop);
                 NIR_PASS(progress, s, nir_opt_remove_phis);
                 NIR_PASS(progress, s, nir_opt_dce);
diff --git a/src/gallium/drivers/zink/zink_compiler.c b/src/gallium/drivers/zink/zink_compiler.c
index 46bd429712a40..ab06c311a3583 100644
--- a/src/gallium/drivers/zink/zink_compiler.c
+++ b/src/gallium/drivers/zink/zink_compiler.c
@@ -1582,7 +1582,7 @@ optimize_nir(struct nir_shader *s, struct zink_shader *zs, bool can_shrink)
       }
       NIR_PASS(progress, s, nir_opt_dce);
       NIR_PASS(progress, s, nir_opt_dead_cf);
-      NIR_PASS(progress, s, nir_lower_phis_to_scalar, false);
+      NIR_PASS(progress, s, nir_lower_phis_to_scalar, NULL, NULL);
       NIR_PASS(progress, s, nir_opt_cse);
 
       nir_opt_peephole_select_options peephole_select_options = {
@@ -3298,7 +3298,7 @@ lower_64bit_vars(nir_shader *shader, bool doubles_only)
    ralloc_free(derefs);
    if (progress) {
       nir_lower_alu_to_scalar(shader, filter_64_bit_instr, NULL);
-      nir_lower_phis_to_scalar(shader, false);
+      nir_lower_phis_to_scalar(shader, NULL, NULL);
       optimize_nir(shader, NULL, true);
    }
    return progress;
diff --git a/src/gallium/frontends/rusticl/core/kernel.rs b/src/gallium/frontends/rusticl/core/kernel.rs
index 1d989145eab09..7e0cf3a0326bb 100644
--- a/src/gallium/frontends/rusticl/core/kernel.rs
+++ b/src/gallium/frontends/rusticl/core/kernel.rs
@@ -591,7 +591,7 @@ fn opt_nir(nir: &mut NirShader, dev: &Device, has_explicit_types: bool) {
                 nir_options.lower_to_scalar_filter,
                 ptr::null(),
             );
-            nir_pass!(nir, nir_lower_phis_to_scalar, false);
+            nir_pass!(nir, nir_lower_phis_to_scalar, NULL, NULL);
         }
 
         progress |= nir_pass!(nir, nir_opt_deref);
diff --git a/src/intel/compiler/brw_nir.c b/src/intel/compiler/brw_nir.c
index a6f39a4bf2ee7..74114c54e237e 100644
--- a/src/intel/compiler/brw_nir.c
+++ b/src/intel/compiler/brw_nir.c
@@ -959,7 +959,7 @@ brw_nir_optimize(nir_shader *nir,
 
       LOOP_OPT(nir_copy_prop);
 
-      LOOP_OPT(nir_lower_phis_to_scalar, false);
+      LOOP_OPT(nir_lower_phis_to_scalar, NULL, NULL);
 
       LOOP_OPT(nir_copy_prop);
       LOOP_OPT(nir_opt_dce);
diff --git a/src/intel/compiler/brw_spirv.c b/src/intel/compiler/brw_spirv.c
index 412256cd3761e..7a4d8067b5745 100644
--- a/src/intel/compiler/brw_spirv.c
+++ b/src/intel/compiler/brw_spirv.c
@@ -27,7 +27,7 @@ optimize(nir_shader *nir)
 
       NIR_PASS(progress, nir, nir_copy_prop);
       NIR_PASS(progress, nir, nir_opt_remove_phis);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, nir, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, nir, nir_opt_dce);
       NIR_PASS(progress, nir, nir_opt_dead_cf);
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/intel/compiler/elk/elk_nir.c b/src/intel/compiler/elk/elk_nir.c
index 06661eebe978d..079d1ff2062f3 100644
--- a/src/intel/compiler/elk/elk_nir.c
+++ b/src/intel/compiler/elk/elk_nir.c
@@ -715,7 +715,7 @@ elk_nir_optimize(nir_shader *nir, bool is_scalar,
       OPT(nir_copy_prop);
 
       if (is_scalar) {
-         OPT(nir_lower_phis_to_scalar, false);
+         OPT(nir_lower_phis_to_scalar, NULL, NULL);
       }
 
       OPT(nir_copy_prop);
diff --git a/src/intel/compiler/elk/elk_spirv.c b/src/intel/compiler/elk/elk_spirv.c
index 786b701492b1f..8ac0382a96a0c 100644
--- a/src/intel/compiler/elk/elk_spirv.c
+++ b/src/intel/compiler/elk/elk_spirv.c
@@ -27,7 +27,7 @@ optimize(nir_shader *nir)
 
       NIR_PASS(progress, nir, nir_copy_prop);
       NIR_PASS(progress, nir, nir_opt_remove_phis);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, nir, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, nir, nir_opt_dce);
       NIR_PASS(progress, nir, nir_opt_dead_cf);
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/mesa/state_tracker/st_glsl_to_nir.cpp b/src/mesa/state_tracker/st_glsl_to_nir.cpp
index db3f0b9d60303..694b264eff7b0 100644
--- a/src/mesa/state_tracker/st_glsl_to_nir.cpp
+++ b/src/mesa/state_tracker/st_glsl_to_nir.cpp
@@ -275,7 +275,7 @@ st_glsl_to_nir_post_opts(struct st_context *st, struct gl_program *prog,
           * vectorize them afterwards again */
          if (!nir->options->lower_to_scalar) {
             NIR_PASS(revectorize, nir, nir_lower_alu_to_scalar, filter_64_bit_instr, nullptr);
-            NIR_PASS(revectorize, nir, nir_lower_phis_to_scalar, false);
+            NIR_PASS(revectorize, nir, nir_lower_phis_to_scalar, NULL, NULL);
          }
          /* doubles lowering requires frexp to be lowered first if it will be,
           * since the pass generates other 64-bit ops.  Most backends lower
diff --git a/src/microsoft/compiler/nir_to_dxil.c b/src/microsoft/compiler/nir_to_dxil.c
index e1e8c93d05ac0..1537f3e4b5860 100644
--- a/src/microsoft/compiler/nir_to_dxil.c
+++ b/src/microsoft/compiler/nir_to_dxil.c
@@ -6349,7 +6349,7 @@ optimize_nir(struct nir_shader *s, const struct nir_to_dxil_options *opts)
       NIR_PASS(progress, s, nir_opt_deref);
       NIR_PASS(progress, s, dxil_nir_lower_upcast_phis, opts->lower_int16 ? 32 : 16);
       NIR_PASS(progress, s, nir_lower_64bit_phis);
-      NIR_PASS(progress, s, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, s, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, s, nir_opt_loop_unroll);
       NIR_PASS(progress, s, nir_lower_pack);
       NIR_PASS(progress, s, dxil_nir_remove_oob_array_accesses);
diff --git a/src/nouveau/compiler/nak_nir.c b/src/nouveau/compiler/nak_nir.c
index 4f6bce9aa12a0..a3f8ae7e90c3f 100644
--- a/src/nouveau/compiler/nak_nir.c
+++ b/src/nouveau/compiler/nak_nir.c
@@ -130,7 +130,7 @@ optimize_nir(nir_shader *nir, const struct nak_compiler *nak, bool allow_copies)
 
       OPT(nir, nir_lower_alu_width, vectorize_filter_cb, NULL);
       OPT(nir, nir_opt_vectorize, vectorize_filter_cb, NULL);
-      OPT(nir, nir_lower_phis_to_scalar, false);
+      OPT(nir, nir_lower_phis_to_scalar, NULL, NULL);
       OPT(nir, nir_lower_frexp);
       OPT(nir, nir_copy_prop);
       OPT(nir, nir_opt_dce);
diff --git a/src/panfrost/clc/pan_compile.c b/src/panfrost/clc/pan_compile.c
index bd22035d2bb1f..9c5aa21b4f15f 100644
--- a/src/panfrost/clc/pan_compile.c
+++ b/src/panfrost/clc/pan_compile.c
@@ -66,7 +66,7 @@ optimize(nir_shader *nir)
 
       NIR_PASS(progress, nir, nir_copy_prop);
       NIR_PASS(progress, nir, nir_opt_remove_phis);
-      NIR_PASS(progress, nir, nir_lower_phis_to_scalar, true);
+      NIR_PASS(progress, nir, nir_lower_all_phis_to_scalar);
       NIR_PASS(progress, nir, nir_opt_dce);
       NIR_PASS(progress, nir, nir_opt_dead_cf);
       NIR_PASS(progress, nir, nir_opt_cse);
diff --git a/src/panfrost/compiler/bifrost_compile.c b/src/panfrost/compiler/bifrost_compile.c
index babaf5c3f1453..b8b4940e4dfd1 100644
--- a/src/panfrost/compiler/bifrost_compile.c
+++ b/src/panfrost/compiler/bifrost_compile.c
@@ -5983,7 +5983,7 @@ bifrost_preprocess_nir(nir_shader *nir, unsigned gpu_id)
 
    NIR_PASS(_, nir, nir_lower_alu_to_scalar, bi_scalarize_filter, NULL);
    NIR_PASS(_, nir, nir_lower_load_const_to_scalar);
-   NIR_PASS(_, nir, nir_lower_phis_to_scalar, true);
+   NIR_PASS(_, nir, nir_lower_all_phis_to_scalar);
    NIR_PASS(_, nir, nir_lower_flrp, 16 | 32 | 64, false /* always_precise */);
    NIR_PASS(_, nir, nir_lower_var_copies);
    NIR_PASS(_, nir, nir_lower_alu);
-- 
GitLab


From e44452bc1bd3eb42ef3196c17c9bb94584bd3e05 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Sun, 29 Jun 2025 10:06:06 +0200
Subject: [PATCH 2/7] aco/isel: refactor emission of bitwise logical operations

---
 src/amd/compiler/aco_builder_h.py             |  14 ++-
 .../aco_select_nir_alu.cpp                    | 103 ++++++------------
 2 files changed, 43 insertions(+), 74 deletions(-)

diff --git a/src/amd/compiler/aco_builder_h.py b/src/amd/compiler/aco_builder_h.py
index c304847b8b419..5881efba878a3 100644
--- a/src/amd/compiler/aco_builder_h.py
+++ b/src/amd/compiler/aco_builder_h.py
@@ -313,10 +313,11 @@ public:
       return Definition(tmp(rc), reg);
    }
 
-   inline aco_opcode w64or32(WaveSpecificOpcode opcode) const {
-      if (program->wave_size == 64)
-         return (aco_opcode) opcode;
+   inline aco_opcode w64(WaveSpecificOpcode opcode) const {
+      return (aco_opcode) opcode;
+   }
 
+   inline aco_opcode w32(WaveSpecificOpcode opcode) const {
       switch (opcode) {
       case s_cselect:
          return aco_opcode::s_cselect_b32;
@@ -361,6 +362,13 @@ public:
       }
    }
 
+   inline aco_opcode w64or32(WaveSpecificOpcode opcode) const {
+      if (program->wave_size == 64)
+         return w64(opcode);
+      else
+         return w32(opcode);
+   }
+
 % for fixed in ['m0', 'vcc', 'exec', 'scc']:
    Operand ${fixed}(Temp tmp) {
        % if fixed == 'vcc' or fixed == 'exec':
diff --git a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
index 6c06f31650a57..67f1abccb9b87 100644
--- a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
+++ b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
@@ -217,30 +217,6 @@ emit_vop2_instruction(isel_context* ctx, nir_alu_instr* instr, aco_opcode opc, T
    }
 }
 
-void
-emit_vop2_instruction_logic64(isel_context* ctx, nir_alu_instr* instr, aco_opcode op, Temp dst)
-{
-   Builder bld = create_alu_builder(ctx, instr);
-
-   Temp src0 = get_alu_src(ctx, instr->src[0]);
-   Temp src1 = get_alu_src(ctx, instr->src[1]);
-
-   if (src1.type() == RegType::sgpr) {
-      assert(src0.type() == RegType::vgpr);
-      std::swap(src0, src1);
-   }
-
-   Temp src00 = bld.tmp(src0.type(), 1);
-   Temp src01 = bld.tmp(src0.type(), 1);
-   bld.pseudo(aco_opcode::p_split_vector, Definition(src00), Definition(src01), src0);
-   Temp src10 = bld.tmp(v1);
-   Temp src11 = bld.tmp(v1);
-   bld.pseudo(aco_opcode::p_split_vector, Definition(src10), Definition(src11), src1);
-   Temp lo = bld.vop2(op, bld.def(v1), src00, src10);
-   Temp hi = bld.vop2(op, bld.def(v1), src01, src11);
-   bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
-}
-
 void
 emit_vop3a_instruction(isel_context* ctx, nir_alu_instr* instr, aco_opcode op, Temp dst,
                        bool flush_denorms = false, unsigned num_sources = 2, bool swap_srcs = false)
@@ -396,18 +372,39 @@ emit_comparison(isel_context* ctx, nir_alu_instr* instr, Temp dst, aco_opcode v1
 }
 
 void
-emit_boolean_logic(isel_context* ctx, nir_alu_instr* instr, Builder::WaveSpecificOpcode op,
-                   Temp dst)
+emit_bitwise_logic(isel_context* ctx, nir_alu_instr* instr, Temp dst,
+                   Builder::WaveSpecificOpcode op, aco_opcode v32_op)
 {
    Builder bld(ctx->program, ctx->block);
    Temp src0 = get_alu_src(ctx, instr->src[0]);
    Temp src1 = get_alu_src(ctx, instr->src[1]);
 
-   assert(dst.regClass() == bld.lm);
-   assert(src0.regClass() == bld.lm);
-   assert(src1.regClass() == bld.lm);
+   if (instr->def.bit_size == 1) {
+      bld.sop2(op, Definition(dst), bld.def(s1, scc), src0, src1);
+   } else if (dst.regClass() == s1) {
+      bld.sop2(bld.w32(op), Definition(dst), bld.def(s1, scc), src0, src1);
+   } else if (dst.regClass() == s2) {
+      bld.sop2(bld.w64(op), Definition(dst), bld.def(s1, scc), src0, src1);
+   } else {
+      assert(dst.regClass().type() == RegType::vgpr && dst.size() <= 2);
+
+      if (src1.type() == RegType::sgpr) {
+         assert(src0.type() == RegType::vgpr);
+         std::swap(src0, src1);
+      }
 
-   bld.sop2(op, Definition(dst), bld.def(s1, scc), src0, src1);
+      if (dst.size() == 1) {
+         bld.vop2(v32_op, Definition(dst), src0, src1);
+      } else {
+         Temp src00 = bld.tmp(src0.type(), 1), src01 = bld.tmp(src0.type(), 1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(src00), Definition(src01), src0);
+         Temp src10 = bld.tmp(v1), src11 = bld.tmp(v1);
+         bld.pseudo(aco_opcode::p_split_vector, Definition(src10), Definition(src11), src1);
+         Temp lo = bld.vop2(v32_op, bld.def(v1), src00, src10);
+         Temp hi = bld.vop2(v32_op, bld.def(v1), src01, src11);
+         bld.pseudo(aco_opcode::p_create_vector, Definition(dst), lo, hi);
+      }
+   }
 }
 
 void
@@ -985,51 +982,15 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       break;
    }
    case nir_op_ior: {
-      if (instr->def.bit_size == 1) {
-         emit_boolean_logic(ctx, instr, Builder::s_or, dst);
-      } else if (dst.regClass() == v1 || dst.regClass() == v2b || dst.regClass() == v1b) {
-         emit_vop2_instruction(ctx, instr, aco_opcode::v_or_b32, dst, true);
-      } else if (dst.regClass() == v2) {
-         emit_vop2_instruction_logic64(ctx, instr, aco_opcode::v_or_b32, dst);
-      } else if (dst.regClass() == s1) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_or_b32, dst, true);
-      } else if (dst.regClass() == s2) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_or_b64, dst, true);
-      } else {
-         isel_err(&instr->instr, "Unimplemented NIR instr bit size");
-      }
+      emit_bitwise_logic(ctx, instr, dst, Builder::s_or, aco_opcode::v_or_b32);
       break;
    }
    case nir_op_iand: {
-      if (instr->def.bit_size == 1) {
-         emit_boolean_logic(ctx, instr, Builder::s_and, dst);
-      } else if (dst.regClass() == v1 || dst.regClass() == v2b || dst.regClass() == v1b) {
-         emit_vop2_instruction(ctx, instr, aco_opcode::v_and_b32, dst, true);
-      } else if (dst.regClass() == v2) {
-         emit_vop2_instruction_logic64(ctx, instr, aco_opcode::v_and_b32, dst);
-      } else if (dst.regClass() == s1) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_and_b32, dst, true);
-      } else if (dst.regClass() == s2) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_and_b64, dst, true);
-      } else {
-         isel_err(&instr->instr, "Unimplemented NIR instr bit size");
-      }
+      emit_bitwise_logic(ctx, instr, dst, Builder::s_and, aco_opcode::v_and_b32);
       break;
    }
    case nir_op_ixor: {
-      if (instr->def.bit_size == 1) {
-         emit_boolean_logic(ctx, instr, Builder::s_xor, dst);
-      } else if (dst.regClass() == v1 || dst.regClass() == v2b || dst.regClass() == v1b) {
-         emit_vop2_instruction(ctx, instr, aco_opcode::v_xor_b32, dst, true);
-      } else if (dst.regClass() == v2) {
-         emit_vop2_instruction_logic64(ctx, instr, aco_opcode::v_xor_b32, dst);
-      } else if (dst.regClass() == s1) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_xor_b32, dst, true);
-      } else if (dst.regClass() == s2) {
-         emit_sop2_instruction(ctx, instr, aco_opcode::s_xor_b64, dst, true);
-      } else {
-         isel_err(&instr->instr, "Unimplemented NIR instr bit size");
-      }
+      emit_bitwise_logic(ctx, instr, dst, Builder::s_xor, aco_opcode::v_xor_b32);
       break;
    }
    case nir_op_ushr: {
@@ -3584,7 +3545,7 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
    }
    case nir_op_ieq: {
       if (instr->src[0].src.ssa->bit_size == 1)
-         emit_boolean_logic(ctx, instr, Builder::s_xnor, dst);
+         emit_bitwise_logic(ctx, instr, dst, Builder::s_xnor, aco_opcode::num_opcodes);
       else
          emit_comparison(
             ctx, instr, dst, aco_opcode::v_cmp_eq_i16, aco_opcode::v_cmp_eq_i32,
@@ -3594,7 +3555,7 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
    }
    case nir_op_ine: {
       if (instr->src[0].src.ssa->bit_size == 1)
-         emit_boolean_logic(ctx, instr, Builder::s_xor, dst);
+         emit_bitwise_logic(ctx, instr, dst, Builder::s_xor, aco_opcode::num_opcodes);
       else
          emit_comparison(
             ctx, instr, dst, aco_opcode::v_cmp_lg_i16, aco_opcode::v_cmp_lg_i32,
-- 
GitLab


From f9b302c08ae2183ca099fd8f74be5e95087c3dc3 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 27 Jun 2025 08:58:26 +0200
Subject: [PATCH 3/7] aco: allow subdword vector-definitions on some VALU
 instructions

---
 src/amd/compiler/aco_register_allocation.cpp       |  3 ++-
 .../instruction_selection/aco_select_nir_alu.cpp   | 14 +++++++-------
 2 files changed, 9 insertions(+), 8 deletions(-)

diff --git a/src/amd/compiler/aco_register_allocation.cpp b/src/amd/compiler/aco_register_allocation.cpp
index 62153a89a3989..2b51ada1bb57b 100644
--- a/src/amd/compiler/aco_register_allocation.cpp
+++ b/src/amd/compiler/aco_register_allocation.cpp
@@ -692,7 +692,8 @@ DefInfo::get_subdword_definition_info(Program* program, const aco_ptr<Instructio
    }
 
    if (instr->isVALU()) {
-      assert(rc.bytes() <= 2);
+      if (rc.bytes() == 3)
+         rc = v1;
 
       if (can_use_SDWA(gfx_level, instr, false))
          return;
diff --git a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
index 67f1abccb9b87..db2375573ca6f 100644
--- a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
+++ b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
@@ -376,8 +376,8 @@ emit_bitwise_logic(isel_context* ctx, nir_alu_instr* instr, Temp dst,
                    Builder::WaveSpecificOpcode op, aco_opcode v32_op)
 {
    Builder bld(ctx->program, ctx->block);
-   Temp src0 = get_alu_src(ctx, instr->src[0]);
-   Temp src1 = get_alu_src(ctx, instr->src[1]);
+   Temp src0 = get_alu_src(ctx, instr->src[0], instr->def.num_components);
+   Temp src1 = get_alu_src(ctx, instr->src[1], instr->def.num_components);
 
    if (instr->def.bit_size == 1) {
       bld.sop2(op, Definition(dst), bld.def(s1, scc), src0, src1);
@@ -412,8 +412,8 @@ emit_bcsel(isel_context* ctx, nir_alu_instr* instr, Temp dst)
 {
    Builder bld(ctx->program, ctx->block);
    Temp cond = get_alu_src(ctx, instr->src[0]);
-   Temp then = get_alu_src(ctx, instr->src[1]);
-   Temp els = get_alu_src(ctx, instr->src[2]);
+   Temp then = get_alu_src(ctx, instr->src[1], instr->def.num_components);
+   Temp els = get_alu_src(ctx, instr->src[2], instr->def.num_components);
 
    assert(cond.regClass() == bld.lm);
 
@@ -829,9 +829,9 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       break;
    }
    case nir_op_inot: {
-      Temp src = get_alu_src(ctx, instr->src[0]);
-      if (dst.regClass() == v1 || dst.regClass() == v2b || dst.regClass() == v1b) {
-         emit_vop1_instruction(ctx, instr, aco_opcode::v_not_b32, dst);
+      Temp src = get_alu_src(ctx, instr->src[0], instr->def.num_components);
+      if (dst.regClass().type() == RegType::vgpr && dst.size() == 1) {
+         bld.vop1(aco_opcode::v_not_b32, Definition(dst), src);
       } else if (dst.regClass() == v2) {
          Temp lo = bld.tmp(v1), hi = bld.tmp(v1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(lo), Definition(hi), src);
-- 
GitLab


From bd24494fe74017e769bafa3176012beb1a4b3494 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Thu, 26 Jun 2025 17:36:16 +0200
Subject: [PATCH 4/7] radv: don't lower subdword phis to scalar

Totals from 193 (0.24% of 79839) affected shaders: (Navi48)
MaxWaves: 6004 -> 6024 (+0.33%)
Instrs: 169276 -> 166784 (-1.47%); split: -3.01%, +1.53%
CodeSize: 940608 -> 915768 (-2.64%); split: -4.29%, +1.64%
VGPRs: 8012 -> 7716 (-3.69%); split: -3.99%, +0.30%
SpillVGPRs: 185 -> 0 (-inf%)
Scratch: 13568 -> 0 (-inf%)
Latency: 2159787 -> 2147084 (-0.59%); split: -2.86%, +2.28%
InvThroughput: 664022 -> 395859 (-40.38%); split: -42.59%, +2.21%
VClause: 2998 -> 2880 (-3.94%); split: -4.27%, +0.33%
SClause: 3117 -> 3120 (+0.10%)
Copies: 21290 -> 16278 (-23.54%); split: -24.74%, +1.20%
Branches: 4757 -> 4760 (+0.06%); split: -0.34%, +0.40%
PreSGPRs: 7369 -> 7378 (+0.12%); split: -0.11%, +0.23%
PreVGPRs: 4257 -> 3859 (-9.35%); split: -9.94%, +0.59%
VALU: 83173 -> 79804 (-4.05%); split: -5.68%, +1.63%
SALU: 36672 -> 37318 (+1.76%); split: -0.02%, +1.78%
VMEM: 4012 -> 3762 (-6.23%); split: -6.83%, +0.60%
SMEM: 4300 -> 4303 (+0.07%)
---
 src/amd/common/nir/ac_nir.c                            | 10 ++++++++++
 src/amd/common/nir/ac_nir.h                            |  3 +++
 src/amd/common/nir/ac_nir_lower_ngg.c                  |  2 +-
 src/amd/common/nir/ac_nir_lower_ngg_mesh.c             |  2 +-
 src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c       |  2 +-
 .../compiler/instruction_selection/aco_isel_setup.cpp  |  2 +-
 src/amd/vulkan/radv_shader.c                           |  2 +-
 7 files changed, 18 insertions(+), 5 deletions(-)

diff --git a/src/amd/common/nir/ac_nir.c b/src/amd/common/nir/ac_nir.c
index 52911038ae05b..264525f55f42a 100644
--- a/src/amd/common/nir/ac_nir.c
+++ b/src/amd/common/nir/ac_nir.c
@@ -879,3 +879,13 @@ ac_nir_repack_invocations_in_workgroup(nir_builder *b, nir_def **input_bool,
          nir_mbcnt_amd(b, input_mask[i], wg_repacked_index_base);
    }
 }
+
+uint8_t
+ac_nir_lower_phis_to_scalar_cb(const nir_instr *instr, const void *_)
+{
+   nir_phi_instr *phi = nir_instr_as_phi(instr);
+   if (phi->def.bit_size == 1 || phi->def.bit_size >= 32)
+      return 1;
+
+   return 32 / phi->def.bit_size;
+}
diff --git a/src/amd/common/nir/ac_nir.h b/src/amd/common/nir/ac_nir.h
index 91445d4f24e89..18b540b4a9357 100644
--- a/src/amd/common/nir/ac_nir.h
+++ b/src/amd/common/nir/ac_nir.h
@@ -436,6 +436,9 @@ ac_nir_scalarize_overfetching_loads_callback(const nir_instr *instr, const void
 enum gl_access_qualifier
 ac_nir_get_mem_access_flags(const nir_intrinsic_instr *instr);
 
+uint8_t
+ac_nir_lower_phis_to_scalar_cb(const nir_instr *instr, const void *_);
+
 #ifdef __cplusplus
 }
 #endif
diff --git a/src/amd/common/nir/ac_nir_lower_ngg.c b/src/amd/common/nir/ac_nir_lower_ngg.c
index b3d0801ae4842..10aa69e5d368d 100644
--- a/src/amd/common/nir/ac_nir_lower_ngg.c
+++ b/src/amd/common/nir/ac_nir_lower_ngg.c
@@ -1838,7 +1838,7 @@ ac_nir_lower_ngg_nogs(nir_shader *shader, const ac_nir_lower_ngg_options *option
    nir_lower_vars_to_ssa(shader);
    nir_remove_dead_variables(shader, nir_var_function_temp, NULL);
    nir_lower_alu_to_scalar(shader, NULL, NULL);
-   nir_lower_all_phis_to_scalar(shader);
+   nir_lower_phis_to_scalar(shader, ac_nir_lower_phis_to_scalar_cb, NULL);
 
    if (options->can_cull) {
       /* It's beneficial to redo these opts after splitting the shader. */
diff --git a/src/amd/common/nir/ac_nir_lower_ngg_mesh.c b/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
index 15c51f512398b..68f29da5f804a 100644
--- a/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
+++ b/src/amd/common/nir/ac_nir_lower_ngg_mesh.c
@@ -1437,7 +1437,7 @@ ac_nir_lower_ngg_mesh(nir_shader *shader,
    nir_lower_vars_to_ssa(shader);
    nir_remove_dead_variables(shader, nir_var_function_temp, NULL);
    nir_lower_alu_to_scalar(shader, NULL, NULL);
-   nir_lower_all_phis_to_scalar(shader);
+   nir_lower_phis_to_scalar(shader, ac_nir_lower_phis_to_scalar_cb, NULL);
 
    /* Optimize load_local_invocation_index. When the API workgroup is smaller than the HW workgroup,
     * local_invocation_id isn't initialized for all lanes and we can't perform this optimization for
diff --git a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
index 7a160023a1753..a542cdecac208 100644
--- a/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
+++ b/src/amd/common/nir/ac_nir_lower_tess_io_to_mem.c
@@ -1624,7 +1624,7 @@ ac_nir_lower_hs_outputs_to_mem(nir_shader *shader, const nir_tcs_info *info,
    NIR_PASS(_, shader, nir_lower_vars_to_ssa);
    NIR_PASS(_, shader, nir_remove_dead_variables, nir_var_function_temp, NULL);
    NIR_PASS(_, shader, nir_lower_alu_to_scalar, NULL, NULL);
-   NIR_PASS(_, shader, nir_lower_all_phis_to_scalar);
+   NIR_PASS(_, shader, nir_lower_phis_to_scalar, ac_nir_lower_phis_to_scalar_cb, NULL);
 
    return true;
 }
diff --git a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
index 22d3c2e14e6a5..76a86b73797fd 100644
--- a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
+++ b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
@@ -251,7 +251,7 @@ void
 setup_nir(isel_context* ctx, nir_shader* nir)
 {
    nir_convert_to_lcssa(nir, true, false);
-   if (nir_lower_all_phis_to_scalar(nir)) {
+   if (nir_lower_phis_to_scalar(nir, ac_nir_lower_phis_to_scalar_cb, NULL)) {
       nir_copy_prop(nir);
       nir_opt_dce(nir);
    }
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 36ede34e8df15..5e7b8bd0d3b3a 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -181,7 +181,7 @@ radv_optimize_nir(struct nir_shader *shader, bool optimize_conservatively)
       NIR_LOOP_PASS(_, skip, shader, nir_lower_vars_to_ssa);
 
       NIR_LOOP_PASS(_, skip, shader, nir_lower_alu_width, vectorize_vec2_16bit, NULL);
-      NIR_LOOP_PASS(_, skip, shader, nir_lower_all_phis_to_scalar);
+      NIR_LOOP_PASS(_, skip, shader, nir_lower_phis_to_scalar, ac_nir_lower_phis_to_scalar_cb, NULL);
 
       NIR_LOOP_PASS(progress, skip, shader, nir_copy_prop);
       NIR_LOOP_PASS(progress, skip, shader, nir_opt_remove_phis);
-- 
GitLab


From 64b2204234de30ee59bc3b1393e0e887d3022879 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Fri, 27 Jun 2025 08:29:39 +0200
Subject: [PATCH 5/7] radv: vectorize some integer arithmetic and bcsel with
 scalar condition

Totals from 106 (0.13% of 79839) affected shaders: (Navi48)
Instrs: 131026 -> 130042 (-0.75%); split: -0.82%, +0.07%
CodeSize: 719120 -> 711516 (-1.06%); split: -1.20%, +0.14%
VGPRs: 5244 -> 5232 (-0.23%)
Latency: 2020748 -> 2004602 (-0.80%); split: -0.81%, +0.01%
InvThroughput: 393330 -> 385414 (-2.01%); split: -2.01%, +0.00%
VClause: 2193 -> 2192 (-0.05%)
Copies: 13963 -> 13558 (-2.90%); split: -2.91%, +0.01%
PreVGPRs: 2953 -> 2921 (-1.08%)
VALU: 65595 -> 64835 (-1.16%); split: -1.16%, +0.00%
SALU: 26887 -> 26611 (-1.03%)
VMEM: 2921 -> 3005 (+2.88%)
VOPD: 168 -> 173 (+2.98%)
---
 src/amd/vulkan/radv_pipeline.c | 49 ++++++++++++++++++++++++++++++++--
 src/amd/vulkan/radv_shader.c   |  2 ++
 2 files changed, 49 insertions(+), 2 deletions(-)

diff --git a/src/amd/vulkan/radv_pipeline.c b/src/amd/vulkan/radv_pipeline.c
index 4077aaed7fae4..d06f3841d5ae4 100644
--- a/src/amd/vulkan/radv_pipeline.c
+++ b/src/amd/vulkan/radv_pipeline.c
@@ -252,6 +252,23 @@ ycbcr_conversion_lookup(const void *data, uint32_t set, uint32_t binding, uint32
    return ycbcr_samplers + array_index;
 }
 
+static uint8_t
+max_alu_src_identity_swizzle(const nir_alu_instr *alu, const nir_alu_src *src)
+{
+   uint8_t max_vector = 32 / alu->def.bit_size;
+   if (nir_src_is_const(src->src))
+      return max_vector;
+
+   /* Return the number of correctly swizzled components. */
+   for (unsigned i = 1; i < alu->def.num_components; i++) {
+      if (src->swizzle[i] != src->swizzle[0] + i)
+         /* Ensure that the result is a power of 2. */
+         return MAX2(i & 0x6, 1);
+   }
+
+   return max_vector;
+}
+
 static uint8_t
 opt_vectorize_callback(const nir_instr *instr, const void *_)
 {
@@ -280,10 +297,38 @@ opt_vectorize_callback(const nir_instr *instr, const void *_)
    }
 
    const unsigned bit_size = alu->def.bit_size;
-   if (bit_size != 16)
+   if (bit_size == 16 && aco_nir_op_supports_packed_math_16bit(alu))
+      return 2;
+
+   if (bit_size != 8 && bit_size != 16)
       return 1;
 
-   return aco_nir_op_supports_packed_math_16bit(alu) ? 2 : 1;
+   /* Keep some opcodes vectorized if the operation can be performed as
+    * 32-bit instruction with packed sources. The condition is that the
+    * sources must have identity swizzles. */
+   uint8_t target_width = 32 / bit_size;
+   switch (alu->op) {
+   case nir_op_bcsel:
+      /* Must have scalar condition. */
+      for (unsigned i = 1; i < alu->def.num_components; i++) {
+         if (alu->src[0].swizzle[i] != alu->src[0].swizzle[0])
+            return 1;
+      }
+      for (unsigned idx = 1; idx < 3; idx++)
+         target_width = MIN2(target_width, max_alu_src_identity_swizzle(alu, &alu->src[idx]));
+      break;
+   case nir_op_iand:
+   case nir_op_ior:
+   case nir_op_ixor:
+   case nir_op_inot:
+      for (unsigned idx = 0; idx < nir_op_infos[alu->op].num_inputs; idx++)
+         target_width = MIN2(target_width, max_alu_src_identity_swizzle(alu, &alu->src[idx]));
+      break;
+   default:
+      return 1;
+   }
+
+   return target_width;
 }
 
 static nir_component_mask_t
diff --git a/src/amd/vulkan/radv_shader.c b/src/amd/vulkan/radv_shader.c
index 5e7b8bd0d3b3a..245c6665fd532 100644
--- a/src/amd/vulkan/radv_shader.c
+++ b/src/amd/vulkan/radv_shader.c
@@ -98,6 +98,8 @@ vectorize_vec2_16bit(const nir_instr *instr, const void *_)
    const unsigned bit_size = alu->def.bit_size;
    if (bit_size == 16)
       return 2;
+   else if (bit_size == 8)
+      return 4;
    else
       return 1;
 }
-- 
GitLab


From 68e7a8c49eeae4b8d422fa1e7beebf718008890e Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Sun, 29 Jun 2025 11:02:33 +0200
Subject: [PATCH 6/7] aco: split vectorized bcsel and bitwise logic VGPR
 definitions

This has a slightly negative effect on parallel-rdp, but positively affects FSR4.

Totals from 14 (0.02% of 79839) affected shaders: (Navi48)
Instrs: 63543 -> 63646 (+0.16%); split: -0.01%, +0.17%
CodeSize: 352888 -> 353608 (+0.20%); split: -0.02%, +0.23%
Latency: 1822354 -> 1825036 (+0.15%)
InvThroughput: 364683 -> 365738 (+0.29%); split: -0.04%, +0.32%
Copies: 9299 -> 9363 (+0.69%); split: -0.11%, +0.80%
PreVGPRs: 1381 -> 1394 (+0.94%)
VALU: 34511 -> 34575 (+0.19%); split: -0.03%, +0.21%
---
 src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp | 4 ++++
 1 file changed, 4 insertions(+)

diff --git a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
index db2375573ca6f..eebc99c945578 100644
--- a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
+++ b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
@@ -395,6 +395,7 @@ emit_bitwise_logic(isel_context* ctx, nir_alu_instr* instr, Temp dst,
 
       if (dst.size() == 1) {
          bld.vop2(v32_op, Definition(dst), src0, src1);
+         emit_split_vector(ctx, dst, instr->def.num_components);
       } else {
          Temp src00 = bld.tmp(src0.type(), 1), src01 = bld.tmp(src0.type(), 1);
          bld.pseudo(aco_opcode::p_split_vector, Definition(src00), Definition(src01), src0);
@@ -429,6 +430,8 @@ emit_bcsel(isel_context* ctx, nir_alu_instr* instr, Temp dst)
       } else {
          isel_err(&instr->instr, "Unimplemented NIR instr bit size");
       }
+
+      emit_split_vector(ctx, dst, instr->def.num_components);
       return;
    }
 
@@ -844,6 +847,7 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       } else {
          isel_err(&instr->instr, "Unimplemented NIR instr bit size");
       }
+      emit_split_vector(ctx, dst, instr->def.num_components);
       break;
    }
    case nir_op_iabs: {
-- 
GitLab


From ce0343d2cd3006f8acc71643356858df758aba7b Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Daniel=20Sch=C3=BCrmann?= <daniel@schuermann.dev>
Date: Mon, 30 Jun 2025 09:33:19 +0200
Subject: [PATCH 7/7] aco/isel: allow to select SGPR defs for vectorized bcsel
 and logical operations

No fossil changes.
---
 src/amd/compiler/instruction_selection/aco_isel_setup.cpp | 5 +++--
 1 file changed, 3 insertions(+), 2 deletions(-)

diff --git a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
index 76a86b73797fd..131a635643134 100644
--- a/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
+++ b/src/amd/compiler/instruction_selection/aco_isel_setup.cpp
@@ -5,6 +5,7 @@
  */
 
 #include "aco_instruction_selection.h"
+#include "aco_interface.h"
 
 #include "nir_builder.h"
 #include "nir_control_flow.h"
@@ -400,9 +401,9 @@ init_context(isel_context* ctx, nir_shader* shader)
                nir_alu_instr* alu_instr = nir_instr_as_alu(instr);
                RegType type = RegType::sgpr;
 
-               /* packed 16bit instructions have to be VGPR */
+               /* Packed 16-bit instructions have to be VGPR. */
                if (alu_instr->def.num_components == 2 &&
-                   nir_op_infos[alu_instr->op].output_size == 0)
+                   aco_nir_op_supports_packed_math_16bit(alu_instr))
                   type = RegType::vgpr;
 
                switch (alu_instr->op) {
-- 
GitLab

