From 6f170e17aeee644959bd8e33fafd59b38e620be0 Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Wed, 25 Jun 2025 17:36:37 +0200
Subject: [PATCH 1/2] aco: add a dedicated pass for better float MODE insertion

Foz-DB Navi48:
Totals from 14 (0.02% of 80251) affected shaders:
Instrs: 13998 -> 11684 (-16.53%)
CodeSize: 104464 -> 86260 (-17.43%)
Latency: 108722 -> 106667 (-1.89%)
InvThroughput: 100332 -> 100324 (-0.01%)
VClause: 621 -> 595 (-4.19%); split: -4.99%, +0.81%
VALU: 6875 -> 6871 (-0.06%)
SALU: 3256 -> 1015 (-68.83%)
VOPD: 1328 -> 1332 (+0.30%)

Removes the s_setreg spam in FSR4.
---
 src/amd/compiler/aco_insert_NOPs.cpp       |  46 +--
 src/amd/compiler/aco_insert_fp_mode.cpp    | 333 +++++++++++++++++++++
 src/amd/compiler/aco_interface.cpp         |   2 +
 src/amd/compiler/aco_ir.h                  |   3 +
 src/amd/compiler/aco_lower_to_hw_instr.cpp |  71 -----
 src/amd/compiler/meson.build               |   1 +
 6 files changed, 340 insertions(+), 116 deletions(-)
 create mode 100644 src/amd/compiler/aco_insert_fp_mode.cpp

diff --git a/src/amd/compiler/aco_insert_NOPs.cpp b/src/amd/compiler/aco_insert_NOPs.cpp
index a69847a059267..91291d38afc52 100644
--- a/src/amd/compiler/aco_insert_NOPs.cpp
+++ b/src/amd/compiler/aco_insert_NOPs.cpp
@@ -870,50 +870,6 @@ instr_is_branch(const aco_ptr<Instruction>& instr)
           instr->opcode == aco_opcode::s_getpc_b64 || instr->opcode == aco_opcode::s_call_b64;
 }
 
-bool
-instr_is_vmem_fp_atomic(const aco_ptr<Instruction>& instr)
-{
-   if (!instr->isVMEM() && !instr->isFlatLike())
-      return false;
-
-   switch (instr->opcode) {
-   case aco_opcode::buffer_atomic_fcmpswap:
-   case aco_opcode::buffer_atomic_fmin:
-   case aco_opcode::buffer_atomic_fmax:
-   case aco_opcode::buffer_atomic_fcmpswap_x2:
-   case aco_opcode::buffer_atomic_fmin_x2:
-   case aco_opcode::buffer_atomic_fmax_x2:
-   case aco_opcode::buffer_atomic_add_f32:
-   case aco_opcode::buffer_atomic_pk_add_f16:
-   case aco_opcode::buffer_atomic_pk_add_bf16:
-   case aco_opcode::image_atomic_fcmpswap:
-   case aco_opcode::image_atomic_fmin:
-   case aco_opcode::image_atomic_fmax:
-   case aco_opcode::image_atomic_pk_add_f16:
-   case aco_opcode::image_atomic_pk_add_bf16:
-   case aco_opcode::image_atomic_add_flt:
-   case aco_opcode::flat_atomic_fcmpswap:
-   case aco_opcode::flat_atomic_fmin:
-   case aco_opcode::flat_atomic_fmax:
-   case aco_opcode::flat_atomic_fcmpswap_x2:
-   case aco_opcode::flat_atomic_fmin_x2:
-   case aco_opcode::flat_atomic_fmax_x2:
-   case aco_opcode::flat_atomic_add_f32:
-   case aco_opcode::flat_atomic_pk_add_f16:
-   case aco_opcode::flat_atomic_pk_add_bf16:
-   case aco_opcode::global_atomic_fcmpswap:
-   case aco_opcode::global_atomic_fmin:
-   case aco_opcode::global_atomic_fmax:
-   case aco_opcode::global_atomic_fcmpswap_x2:
-   case aco_opcode::global_atomic_fmin_x2:
-   case aco_opcode::global_atomic_fmax_x2:
-   case aco_opcode::global_atomic_add_f32:
-   case aco_opcode::global_atomic_pk_add_f16:
-   case aco_opcode::global_atomic_pk_add_bf16: return true;
-   default: return false;
-   }
-}
-
 void
 handle_instruction_gfx10(State& state, NOP_ctx_gfx10& ctx, aco_ptr<Instruction>& instr,
                          std::vector<aco_ptr<Instruction>>& new_instructions)
@@ -944,7 +900,7 @@ handle_instruction_gfx10(State& state, NOP_ctx_gfx10& ctx, aco_ptr<Instruction>&
               instr->opcode == aco_opcode::s_waitcnt_lgkmcnt ||
               instr->opcode == aco_opcode::s_wait_idle) {
       ctx.waits_since_fp_atomic = 3;
-   } else if (instr_is_vmem_fp_atomic(instr)) {
+   } else if (instr_is_vmem_fp_atomic(instr.get())) {
       ctx.waits_since_fp_atomic = 0;
    } else {
       ctx.waits_since_fp_atomic += get_wait_states(instr);
diff --git a/src/amd/compiler/aco_insert_fp_mode.cpp b/src/amd/compiler/aco_insert_fp_mode.cpp
new file mode 100644
index 0000000000000..1f11656429424
--- /dev/null
+++ b/src/amd/compiler/aco_insert_fp_mode.cpp
@@ -0,0 +1,333 @@
+/*
+ * Copyright Â© 2025 Valve Corporation
+ *
+ * SPDX-License-Identifier: MIT
+ */
+
+#include "aco_builder.h"
+#include "aco_ir.h"
+
+#include <vector>
+
+namespace aco {
+
+namespace {
+
+enum mode_field : uint8_t {
+   mode_round32 = 0,
+   mode_round16_64,
+   mode_denorm32,
+   mode_denorm16_64,
+   mode_fp16_ovfl,
+
+   mode_field_count,
+};
+
+using mode_mask = uint8_t;
+static_assert(mode_field_count <= sizeof(mode_mask) * 8, "larger mode_mask needed");
+
+struct fp_mode_state {
+   uint8_t fields[mode_field_count] = {};
+   mode_mask dirty = 0; /* BITFIELD_BIT(enum mode_field) */
+
+   fp_mode_state() = default;
+
+   fp_mode_state(float_mode mode)
+   {
+      fields[mode_round32] = mode.round32;
+      fields[mode_round16_64] = mode.round16_64;
+      fields[mode_denorm32] = mode.denorm32;
+      fields[mode_denorm16_64] = mode.denorm16_64;
+      fields[mode_fp16_ovfl] = 0;
+   }
+
+   void join(const fp_mode_state& other)
+   {
+      dirty |= other.dirty;
+      for (unsigned i = 0; i < mode_field_count; i++) {
+         if (fields[i] != other.fields[i])
+            dirty |= BITFIELD_BIT(i);
+      }
+   }
+
+   bool require(mode_field field, uint8_t val)
+   {
+      if (fields[field] == val && !(dirty & BITFIELD_BIT(field)))
+         return false;
+
+      fields[field] = val;
+      dirty |= BITFIELD_BIT(field);
+      return true;
+   }
+
+   uint8_t round() const { return fields[mode_round32] | (fields[mode_round16_64] << 2); }
+
+   uint8_t denorm() const { return fields[mode_denorm32] | (fields[mode_denorm16_64] << 2); }
+};
+
+struct fp_mode_ctx {
+   std::vector<fp_mode_state> block_states;
+   Program* program;
+};
+
+void
+emit_set_mode(Builder& bld, const fp_mode_state& state)
+{
+   bool set_round = state.dirty & (BITFIELD_BIT(mode_round32) | BITFIELD_BIT(mode_round16_64));
+   bool set_denorm = state.dirty & (BITFIELD_BIT(mode_denorm32) | BITFIELD_BIT(mode_denorm16_64));
+   bool set_fp16_ovfl = state.dirty & BITFIELD_BIT(mode_fp16_ovfl);
+
+   if (bld.program->gfx_level >= GFX10) {
+      if (set_round)
+         bld.sopp(aco_opcode::s_round_mode, state.round());
+      if (set_denorm)
+         bld.sopp(aco_opcode::s_denorm_mode, state.denorm());
+   } else if (set_round || set_denorm) {
+      /* "((size - 1) << 11) | register" (MODE is encoded as register 1) */
+      uint8_t val = state.round() | (state.denorm() << 4);
+      bld.sopk(aco_opcode::s_setreg_imm32_b32, Operand::literal32(val), (7 << 11) | 1);
+   }
+
+   if (set_fp16_ovfl) {
+      /* "((size - 1) << 11 | (offset << 6) | register" (MODE is encoded as register 1, we
+       * want to set a single bit at offset 23)
+       */
+      bld.sopk(aco_opcode::s_setreg_imm32_b32, Operand::literal32(state.fields[mode_fp16_ovfl]),
+               (0 << 11) | (23 << 6) | 1);
+   }
+}
+
+mode_mask
+vmem_default_needs(Instruction* instr)
+{
+   switch (instr->opcode) {
+   case aco_opcode::buffer_atomic_fcmpswap:
+   case aco_opcode::buffer_atomic_fmin:
+   case aco_opcode::buffer_atomic_fmax:
+   case aco_opcode::buffer_atomic_add_f32:
+   case aco_opcode::flat_atomic_fcmpswap:
+   case aco_opcode::flat_atomic_fmin:
+   case aco_opcode::flat_atomic_fmax:
+   case aco_opcode::flat_atomic_add_f32:
+   case aco_opcode::global_atomic_fcmpswap:
+   case aco_opcode::global_atomic_fmin:
+   case aco_opcode::global_atomic_fmax:
+   case aco_opcode::global_atomic_add_f32:
+   case aco_opcode::image_atomic_fcmpswap:
+   case aco_opcode::image_atomic_fmin:
+   case aco_opcode::image_atomic_fmax:
+   case aco_opcode::image_atomic_add_flt: return BITFIELD_BIT(mode_denorm32);
+   case aco_opcode::buffer_atomic_fcmpswap_x2:
+   case aco_opcode::buffer_atomic_fmin_x2:
+   case aco_opcode::buffer_atomic_fmax_x2:
+   case aco_opcode::buffer_atomic_pk_add_f16:
+   case aco_opcode::buffer_atomic_pk_add_bf16:
+   case aco_opcode::flat_atomic_fcmpswap_x2:
+   case aco_opcode::flat_atomic_fmin_x2:
+   case aco_opcode::flat_atomic_fmax_x2:
+   case aco_opcode::flat_atomic_pk_add_f16:
+   case aco_opcode::flat_atomic_pk_add_bf16:
+   case aco_opcode::global_atomic_fcmpswap_x2:
+   case aco_opcode::global_atomic_fmin_x2:
+   case aco_opcode::global_atomic_fmax_x2:
+   case aco_opcode::global_atomic_pk_add_f16:
+   case aco_opcode::global_atomic_pk_add_bf16:
+   case aco_opcode::image_atomic_pk_add_f16:
+   case aco_opcode::image_atomic_pk_add_bf16: return BITFIELD_BIT(mode_denorm16_64);
+   default: return 0;
+   }
+}
+
+mode_mask
+instr_default_needs(fp_mode_ctx* ctx, Block* block, Instruction* instr)
+{
+   if ((instr->isVMEM() || instr->isFlatLike()) && ctx->program->gfx_level < GFX12)
+      return vmem_default_needs(instr);
+
+   switch (instr->opcode) {
+   case aco_opcode::s_branch:
+   case aco_opcode::s_cbranch_scc0:
+   case aco_opcode::s_cbranch_scc1:
+   case aco_opcode::s_cbranch_vccz:
+   case aco_opcode::s_cbranch_vccnz:
+   case aco_opcode::s_cbranch_execz:
+   case aco_opcode::s_cbranch_execnz:
+      if (instr->salu().imm > block->index)
+         return 0;
+      FALLTHROUGH;
+   case aco_opcode::s_swappc_b64:
+   case aco_opcode::s_setpc_b64:
+   case aco_opcode::s_call_b64:
+      /* Restore defaults on loop back edges and calls. */
+      return BITFIELD_MASK(mode_field_count);
+   case aco_opcode::ds_cmpst_f32:
+   case aco_opcode::ds_min_f32:
+   case aco_opcode::ds_max_f32:
+   case aco_opcode::ds_add_f32:
+   case aco_opcode::ds_min_src2_f32:
+   case aco_opcode::ds_max_src2_f32:
+   case aco_opcode::ds_add_src2_f32:
+   case aco_opcode::ds_cmpst_rtn_f32:
+   case aco_opcode::ds_min_rtn_f32:
+   case aco_opcode::ds_max_rtn_f32:
+   case aco_opcode::ds_add_rtn_f32: return BITFIELD_BIT(mode_denorm32);
+   case aco_opcode::ds_cmpst_f64:
+   case aco_opcode::ds_min_f64:
+   case aco_opcode::ds_max_f64:
+   case aco_opcode::ds_min_src2_f64:
+   case aco_opcode::ds_max_src2_f64:
+   case aco_opcode::ds_cmpst_rtn_f64:
+   case aco_opcode::ds_min_rtn_f64:
+   case aco_opcode::ds_max_rtn_f64:
+   case aco_opcode::ds_pk_add_f16:
+   case aco_opcode::ds_pk_add_rtn_f16:
+   case aco_opcode::ds_pk_add_bf16:
+   case aco_opcode::ds_pk_add_rtn_bf16: return BITFIELD_BIT(mode_denorm16_64);
+   case aco_opcode::v_cvt_pk_u8_f32: return BITFIELD_BIT(mode_round32);
+   default: break;
+   }
+
+   if (!instr->isVALU() && !instr->isSALU() && !instr->isVINTRP())
+      return 0;
+   if (instr->definitions.empty())
+      return 0;
+
+   const aco_alu_opcode_info& info = instr_info.alu_opcode_infos[(int)instr->opcode];
+
+   mode_mask res = 0;
+
+   for (unsigned i = 0; i < info.num_operands; i++) {
+      aco_type type = info.op_types[i];
+      if (type.base_type != aco_base_type_float && type.base_type != aco_base_type_bfloat)
+         continue;
+
+      if (type.bit_size == 32)
+         res |= BITFIELD_BIT(mode_denorm32);
+      else if (type.bit_size >= 16)
+         res |= BITFIELD_BIT(mode_denorm16_64);
+   }
+
+   aco_type type = info.def_types[0];
+   if (type.base_type == aco_base_type_float || type.base_type == aco_base_type_bfloat) {
+      if (type.bit_size == 32)
+         res |= BITFIELD_BIT(mode_denorm32) | BITFIELD_BIT(mode_round32);
+      else if (type.bit_size >= 16)
+         res |= BITFIELD_BIT(mode_denorm16_64) | BITFIELD_BIT(mode_round16_64);
+
+      if (type.bit_size <= 16)
+         res |= BITFIELD_BIT(mode_fp16_ovfl);
+   }
+
+   if (instr->opcode == aco_opcode::v_fma_mixlo_f16 || instr->opcode == aco_opcode::v_fma_mixlo_f16)
+      res |= BITFIELD_BIT(mode_round32);
+   else if (instr->opcode == aco_opcode::v_fma_mix_f32 && instr->valu().opsel_hi)
+      res |= BITFIELD_BIT(mode_denorm16_64);
+
+   return res;
+}
+
+void
+emit_set_mode_block(fp_mode_ctx* ctx, Block* block)
+{
+   Builder bld(ctx->program, block);
+   fp_mode_state fp_state;
+   const fp_mode_state default_state(block->fp_mode);
+
+   if (block->index == 0) {
+      bool inital_unknown = (ctx->program->info.merged_shader_compiled_separately &&
+                             ctx->program->stage.sw == SWStage::GS) ||
+                            (ctx->program->info.merged_shader_compiled_separately &&
+                             ctx->program->stage.sw == SWStage::TCS);
+
+      if (inital_unknown) {
+         fp_state.dirty = BITFIELD_MASK(mode_field_count) & ~BITFIELD_BIT(mode_fp16_ovfl);
+      } else {
+         float_mode program_mode;
+         program_mode.val = ctx->program->config->float_mode;
+         fp_state = fp_mode_state(program_mode);
+      }
+   } else if (block->linear_preds.empty()) {
+      fp_state = default_state;
+   } else {
+      assert(block->linear_preds[0] < block->index);
+      fp_state = ctx->block_states[block->linear_preds[0]];
+      for (unsigned i = 1; i < block->linear_preds.size(); i++) {
+         unsigned pred = block->linear_preds[i];
+         fp_mode_state other = pred < block->index
+                                  ? ctx->block_states[pred]
+                                  : fp_mode_state(ctx->program->blocks[pred].fp_mode);
+         fp_state.join(other);
+      }
+   }
+
+   /* If we don't know the value, set it to the default one next time. */
+   u_foreach_bit (field, fp_state.dirty)
+      fp_state.fields[field] = default_state.fields[field];
+
+   for (std::vector<aco_ptr<Instruction>>::iterator it = block->instructions.begin();
+        it < block->instructions.end(); ++it) {
+      bool set_mode = false;
+
+      Instruction* instr = it->get();
+
+      if (instr->opcode == aco_opcode::p_v_cvt_f16_f32_rtne ||
+          instr->opcode == aco_opcode::p_s_cvt_f16_f32_rtne) {
+         set_mode |= fp_state.require(mode_round16_64, fp_round_ne);
+         set_mode |= fp_state.require(mode_fp16_ovfl, default_state.fields[mode_fp16_ovfl]);
+         set_mode |= fp_state.require(mode_denorm16_64, default_state.fields[mode_denorm16_64]);
+         if (instr->opcode == aco_opcode::p_v_cvt_f16_f32_rtne)
+            instr->opcode = aco_opcode::v_cvt_f16_f32;
+         else
+            instr->opcode = aco_opcode::s_cvt_f16_f32;
+      } else if (instr->opcode == aco_opcode::p_v_cvt_pk_fp8_f32_ovfl) {
+         set_mode |= fp_state.require(mode_fp16_ovfl, 1);
+         instr->opcode = aco_opcode::v_cvt_pk_fp8_f32;
+      } else {
+         mode_mask default_needs = instr_default_needs(ctx, block, instr);
+         u_foreach_bit (i, default_needs)
+            set_mode |= fp_state.require((mode_field)i, default_state.fields[i]);
+      }
+
+      if (set_mode) {
+         bld.reset(&block->instructions, it);
+         emit_set_mode(bld, fp_state);
+         fp_state.dirty = 0;
+         /* Update the iterator if it was invalidated */
+         it = bld.it;
+      }
+   }
+
+   if (block->kind & block_kind_end_with_regs) {
+      /* Restore default. */
+      for (unsigned i = 0; i < mode_field_count; i++)
+         fp_state.require((mode_field)i, default_state.fields[i]);
+      if (fp_state.dirty) {
+         bld.reset(block);
+         emit_set_mode(bld, fp_state);
+         fp_state.dirty = 0;
+      }
+   }
+
+   ctx->block_states[block->index] = fp_state;
+}
+
+} // namespace
+
+bool
+instr_is_vmem_fp_atomic(Instruction* instr)
+{
+   return vmem_default_needs(instr) != 0;
+}
+
+void
+insert_fp_mode(Program* program)
+{
+   fp_mode_ctx ctx;
+   ctx.program = program;
+   ctx.block_states.resize(program->blocks.size());
+
+   for (Block& block : program->blocks)
+      emit_set_mode_block(&ctx, &block);
+}
+
+} // namespace aco
diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index a80a8189b65b6..c5877c04415a9 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -157,6 +157,8 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
    if (!options->optimisations_disabled && !(debug_flags & DEBUG_NO_SCHED_ILP))
       schedule_ilp(program.get());
 
+   insert_fp_mode(program.get());
+
    insert_waitcnt(program.get());
    insert_NOPs(program.get());
    if (program->gfx_level >= GFX11)
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index bfa0427bebed5..bc19a46c224ce 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -1908,6 +1908,8 @@ unsigned get_vopd_opy_start(const Instruction* instr);
 
 bool should_form_clause(const Instruction* a, const Instruction* b);
 
+bool instr_is_vmem_fp_atomic(Instruction* instr);
+
 enum vmem_type : uint8_t {
    vmem_nosampler = 1 << 0,
    vmem_sampler = 1 << 1,
@@ -2286,6 +2288,7 @@ void lower_to_hw_instr(Program* program);
 void schedule_program(Program* program);
 void schedule_ilp(Program* program);
 void schedule_vopd(Program* program);
+void insert_fp_mode(Program* program);
 void spill(Program* program);
 void insert_waitcnt(Program* program);
 void insert_delay_alu(Program* program);
diff --git a/src/amd/compiler/aco_lower_to_hw_instr.cpp b/src/amd/compiler/aco_lower_to_hw_instr.cpp
index 12f582be83347..5fd17e2813455 100644
--- a/src/amd/compiler/aco_lower_to_hw_instr.cpp
+++ b/src/amd/compiler/aco_lower_to_hw_instr.cpp
@@ -2123,46 +2123,6 @@ handle_operands_linear_vgpr(std::map<PhysReg, copy_operation>& copy_map, lower_c
    ctx->program->statistics.copies += scratch_sgpr == scc ? 2 : 4;
 }
 
-void
-emit_set_mode(Builder& bld, float_mode new_mode, bool set_round, bool set_denorm)
-{
-   if (bld.program->gfx_level >= GFX10) {
-      if (set_round)
-         bld.sopp(aco_opcode::s_round_mode, new_mode.round);
-      if (set_denorm)
-         bld.sopp(aco_opcode::s_denorm_mode, new_mode.denorm);
-   } else if (set_round || set_denorm) {
-      /* "((size - 1) << 11) | register" (MODE is encoded as register 1) */
-      bld.sopk(aco_opcode::s_setreg_imm32_b32, Operand::literal32(new_mode.val), (7 << 11) | 1);
-   }
-}
-
-void
-emit_set_mode_from_block(Builder& bld, Program& program, Block* block)
-{
-   float_mode initial;
-   initial.val = program.config->float_mode;
-
-   bool inital_unknown =
-      (program.info.merged_shader_compiled_separately && program.stage.sw == SWStage::GS) ||
-      (program.info.merged_shader_compiled_separately && program.stage.sw == SWStage::TCS);
-   bool is_start = block->index == 0;
-   bool set_round = is_start && (inital_unknown || block->fp_mode.round != initial.round);
-   bool set_denorm = is_start && (inital_unknown || block->fp_mode.denorm != initial.denorm);
-   if (block->kind & block_kind_top_level) {
-      for (unsigned pred : block->linear_preds) {
-         if (program.blocks[pred].fp_mode.round != block->fp_mode.round)
-            set_round = true;
-         if (program.blocks[pred].fp_mode.denorm != block->fp_mode.denorm)
-            set_denorm = true;
-      }
-   }
-   /* only allow changing modes at top-level blocks so this doesn't break
-    * the "jump over empty blocks" optimization */
-   assert((!set_round && !set_denorm) || (block->kind & block_kind_top_level));
-   emit_set_mode(bld, block->fp_mode, set_round, set_denorm);
-}
-
 void
 lower_image_sample(lower_context* ctx, aco_ptr<Instruction>& instr)
 {
@@ -2279,8 +2239,6 @@ lower_to_hw_instr(Program* program)
       ctx.instructions.reserve(block->instructions.size());
       Builder bld(program, &ctx.instructions);
 
-      emit_set_mode_from_block(bld, *program, block);
-
       for (size_t instr_idx = 0; instr_idx < block->instructions.size(); instr_idx++) {
          aco_ptr<Instruction>& instr = block->instructions[instr_idx];
 
@@ -2927,35 +2885,6 @@ lower_to_hw_instr(Program* program)
             } else if (emit_s_barrier) {
                bld.sopp(aco_opcode::s_barrier);
             }
-         } else if (instr->opcode == aco_opcode::p_v_cvt_f16_f32_rtne ||
-                    instr->opcode == aco_opcode::p_s_cvt_f16_f32_rtne) {
-            float_mode new_mode = block->fp_mode;
-            new_mode.round16_64 = fp_round_ne;
-            bool set_round = new_mode.round != block->fp_mode.round;
-
-            emit_set_mode(bld, new_mode, set_round, false);
-
-            if (instr->opcode == aco_opcode::p_v_cvt_f16_f32_rtne)
-               instr->opcode = aco_opcode::v_cvt_f16_f32;
-            else
-               instr->opcode = aco_opcode::s_cvt_f16_f32;
-            ctx.instructions.emplace_back(std::move(instr));
-
-            emit_set_mode(bld, block->fp_mode, set_round, false);
-         } else if (instr->opcode == aco_opcode::p_v_cvt_pk_fp8_f32_ovfl) {
-            /* FP8/BF8 uses FP16_OVFL(1) to clamp to max finite result. Temporarily set it for the
-             * instruction.
-             * "((size - 1) << 11 | (offset << 6) | register" (MODE is encoded as register 1, we
-             * want to set a single bit at offset 23)
-             */
-            bld.sopk(aco_opcode::s_setreg_imm32_b32, Operand::literal32(1),
-                     (0 << 11) | (23 << 6) | 1);
-
-            instr->opcode = aco_opcode::v_cvt_pk_fp8_f32;
-            ctx.instructions.emplace_back(std::move(instr));
-
-            bld.sopk(aco_opcode::s_setreg_imm32_b32, Operand::literal32(0),
-                     (0 << 11) | (23 << 6) | 1);
          } else if (instr->isMIMG() && instr->mimg().strict_wqm) {
             lower_image_sample(&ctx, instr);
             ctx.instructions.emplace_back(std::move(instr));
diff --git a/src/amd/compiler/meson.build b/src/amd/compiler/meson.build
index a85b54768b39f..db32444a07924 100644
--- a/src/amd/compiler/meson.build
+++ b/src/amd/compiler/meson.build
@@ -53,6 +53,7 @@ libaco_files = files(
   'aco_form_hard_clauses.cpp',
   'aco_insert_delay_alu.cpp',
   'aco_insert_exec_mask.cpp',
+  'aco_insert_fp_mode.cpp',
   'aco_insert_NOPs.cpp',
   'aco_insert_waitcnt.cpp',
   'aco_reduce_assign.cpp',
-- 
GitLab


From df99ffe984ff929fac16a7bbde7131614bb7480c Mon Sep 17 00:00:00 2001
From: Georg Lehmann <dadschoorse@gmail.com>
Date: Mon, 30 Jun 2025 16:11:42 +0200
Subject: [PATCH 2/2] aco: only insert fp mode when needed

---
 src/amd/compiler/aco_interface.cpp            |  3 ++-
 src/amd/compiler/aco_ir.cpp                   |  1 +
 src/amd/compiler/aco_ir.h                     |  1 +
 .../instruction_selection/aco_select_nir.cpp  |  2 ++
 .../aco_select_nir_alu.cpp                    | 19 +++++++++++++------
 5 files changed, 19 insertions(+), 7 deletions(-)

diff --git a/src/amd/compiler/aco_interface.cpp b/src/amd/compiler/aco_interface.cpp
index c5877c04415a9..fe1249f4591d3 100644
--- a/src/amd/compiler/aco_interface.cpp
+++ b/src/amd/compiler/aco_interface.cpp
@@ -157,7 +157,8 @@ aco_postprocess_shader(const struct aco_compiler_options* options,
    if (!options->optimisations_disabled && !(debug_flags & DEBUG_NO_SCHED_ILP))
       schedule_ilp(program.get());
 
-   insert_fp_mode(program.get());
+   if (program->needs_fp_mode_insertion)
+      insert_fp_mode(program.get());
 
    insert_waitcnt(program.get());
    insert_NOPs(program.get());
diff --git a/src/amd/compiler/aco_ir.cpp b/src/amd/compiler/aco_ir.cpp
index 7e14c28ba4324..aee4ba133c7d1 100644
--- a/src/amd/compiler/aco_ir.cpp
+++ b/src/amd/compiler/aco_ir.cpp
@@ -233,6 +233,7 @@ init_program(Program* program, Stage stage, const struct aco_shader_info* info,
    program->next_fp_mode.denorm32 = 0;
    program->next_fp_mode.round16_64 = fp_round_ne;
    program->next_fp_mode.round32 = fp_round_ne;
+   program->needs_fp_mode_insertion = false;
 }
 
 bool
diff --git a/src/amd/compiler/aco_ir.h b/src/amd/compiler/aco_ir.h
index bc19a46c224ce..435608489535a 100644
--- a/src/amd/compiler/aco_ir.h
+++ b/src/amd/compiler/aco_ir.h
@@ -2142,6 +2142,7 @@ public:
    Stage stage;
    bool needs_exact = false; /* there exists an instruction with disable_wqm = true */
    bool needs_wqm = false;   /* there exists a p_wqm instruction */
+   bool needs_fp_mode_insertion = false; /* insert_fp_mode should be run */
    bool has_smem_buffer_or_global_loads = false;
    bool has_pops_overlapped_waves_wait = false;
    bool has_color_exports = false;
diff --git a/src/amd/compiler/instruction_selection/aco_select_nir.cpp b/src/amd/compiler/instruction_selection/aco_select_nir.cpp
index ec90116e6a621..7586ce5181886 100644
--- a/src/amd/compiler/instruction_selection/aco_select_nir.cpp
+++ b/src/amd/compiler/instruction_selection/aco_select_nir.cpp
@@ -1429,6 +1429,7 @@ select_program(Program* program, unsigned shader_count, struct nir_shader* const
       return select_program_rt(ctx, shader_count, shaders, args);
 
    if (shader_count >= 2) {
+      program->needs_fp_mode_insertion = true;
       select_program_merged(ctx, shader_count, shaders);
    } else {
       bool need_barrier = false, check_merged_wave_info = false, endif_merged_wave_info = false;
@@ -1437,6 +1438,7 @@ select_program(Program* program, unsigned shader_count, struct nir_shader* const
       /* Handle separate compilation of VS+TCS and {VS,TES}+GS on GFX9+. */
       if (ctx.program->info.merged_shader_compiled_separately) {
          assert(ctx.program->gfx_level >= GFX9);
+         program->needs_fp_mode_insertion = true;
          if (ctx.stage.sw == SWStage::VS || ctx.stage.sw == SWStage::TES) {
             check_merged_wave_info = endif_merged_wave_info = true;
          } else {
diff --git a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
index 6c06f31650a57..e841021997d91 100644
--- a/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
+++ b/src/amd/compiler/instruction_selection/aco_select_nir_alu.cpp
@@ -2497,9 +2497,10 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       }
       Temp src = get_alu_src(ctx, instr->src[0]);
       if (instr->op == nir_op_f2f16_rtne && ctx->block->fp_mode.round16_64 != fp_round_ne) {
-         /* We emit s_round_mode/s_setreg_imm32 in lower_to_hw_instr to
-          * keep value numbering and the scheduler simpler.
+         /* We emit s_round_mode/s_setreg_imm32 in insert_fp_mode to
+          * keep value numbering and scheduling simpler.
           */
+         ctx->program->needs_fp_mode_insertion = true;
          if (dst.regClass() == v2b)
             bld.vop1(aco_opcode::p_v_cvt_f16_f32_rtne, Definition(dst), src);
          else
@@ -2587,6 +2588,8 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
          }
       }
 
+      ctx->program->needs_fp_mode_insertion |= instr->op == nir_op_f2e4m3fn_satfn;
+
       aco_opcode opcode = instr->op == nir_op_f2e4m3fn || instr->op == nir_op_f2e4m3fn_sat
                              ? aco_opcode::v_cvt_pk_fp8_f32
                           : instr->op == nir_op_f2e4m3fn_satfn ? aco_opcode::p_v_cvt_pk_fp8_f32_ovfl
@@ -3202,10 +3205,12 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
       Temp src = get_alu_src(ctx, instr->src[0]);
       if (dst.regClass() == v1) {
          Temp f16;
-         if (ctx->block->fp_mode.round16_64 != fp_round_ne)
+         if (ctx->block->fp_mode.round16_64 != fp_round_ne) {
+            ctx->program->needs_fp_mode_insertion = true;
             f16 = bld.vop1(aco_opcode::p_v_cvt_f16_f32_rtne, bld.def(v2b), src);
-         else
+         } else {
             f16 = bld.vop1(aco_opcode::v_cvt_f16_f32, bld.def(v2b), src);
+         }
 
          if (ctx->block->fp_mode.denorm16_64 != fp_denorm_keep) {
             bld.vop1(aco_opcode::v_cvt_f32_f16, Definition(dst), f16);
@@ -3241,10 +3246,12 @@ visit_alu_instr(isel_context* ctx, nir_alu_instr* instr)
          }
       } else if (dst.regClass() == s1) {
          Temp f16;
-         if (ctx->block->fp_mode.round16_64 != fp_round_ne)
+         if (ctx->block->fp_mode.round16_64 != fp_round_ne) {
+            ctx->program->needs_fp_mode_insertion = true;
             f16 = bld.sop1(aco_opcode::p_s_cvt_f16_f32_rtne, bld.def(s1), src);
-         else
+         } else {
             f16 = bld.sop1(aco_opcode::s_cvt_f16_f32, bld.def(s1), src);
+         }
 
          if (ctx->block->fp_mode.denorm16_64 != fp_denorm_keep) {
             bld.sop1(aco_opcode::s_cvt_f32_f16, Definition(dst), f16);
-- 
GitLab

